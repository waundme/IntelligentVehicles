{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waundme/IntelligentVehicles/blob/main/Intelligent_Vehicles_und_Grobid_im_Batch_Mode%2C_XML_TEI_und_ElasticSearch%2C_einfachstes_Setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sci-Hub"
      ],
      "metadata": {
        "id": "Ij48uty9u4Vz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sci-Hub-DOIs herunterladen und entpacken.\n",
        "!wget https://www.sci-hub.st/datasets/dois-2022-02-12.7z\n",
        "!7za x dois-2022-02-12.7z"
      ],
      "metadata": {
        "id": "Gu4iPOeSXIwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alle Intelligent-Vehicle-DOIs rausgreppen und in dois.txt speichern.\n",
        "!grep -a 10.1109/tiv /content/sci-hub-doi-2022-02-12.txt > dois.txt"
      ],
      "metadata": {
        "id": "p-FIcM6cX-Hl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# scitopdf"
      ],
      "metadata": {
        "id": "3IQOGQpeu_lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scitopdf installieren\n",
        "!git clone https://github.com/dougy147/scitopdf\n",
        "!cd scitopdf && make install"
      ],
      "metadata": {
        "id": "hmUkY_fmZGJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alle Intelligent-Vehicle-PDFs herunterladen und in /content/input speichern. \n",
        "# Bei Paper 322 scheint es einen Fehler zu geben und Script hängt, läuft aber schließlich durch.\n",
        "# Möglicherweise Parameter -q (quiet) hinzufügen, damit der Output-Buffer hier nicht volläuft.\n",
        "#!scitopdf -q -D /content/input -l dois.txt\n",
        "!scitopdf -D /content/input -l dois.txt"
      ],
      "metadata": {
        "id": "IOIop1csZYY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GrobId"
      ],
      "metadata": {
        "id": "rVsqcP72vF_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GrobId herunterladen und entpacken.\n",
        "!wget https://github.com/kermitt2/grobid/archive/0.7.2.zip\n",
        "!unzip 0.7.2.zip"
      ],
      "metadata": {
        "id": "vtPTg58vG7NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GrobId bauen und installieren. Das dauert 4 Minuten oder so, weil überkompliziertes Java-Build-System.\n",
        "!cd /content/grobid-0.7.2 && ./gradlew clean install"
      ],
      "metadata": {
        "id": "OowjzRYFHS1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjKJaEmdyh3z"
      },
      "source": [
        "# Output-Directory anlegen.\n",
        "import os\n",
        "os.mkdir(\"/content/output\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GrobId im Batch-Modus. Nimmt alle PDFs im Input-Verzeichnis und konvertiert sie nach XML/TEI im Output-Verzeichnis.\n",
        "# Gibt ein paar Fehler, ich glaube, weil einige PDFs leer sind, keinen Inhalt haben.\n",
        "# Braucht eine halbe Stunde oder so.\n",
        "# -Xmx4G allokiert der JVM 4 GB RAM, möglicherweise muß man das höher setzen, Log/Output beobachten.\n",
        "!java -Xmx4G -jar /content/grobid-0.7.2/grobid-core/build/libs/grobid-core-0.7.2-onejar.jar -gH /content/grobid-0.7.2/grobid-home -dIn /content/input/ -dOut /content/output -exe processFullText "
      ],
      "metadata": {
        "id": "w2Td__WLJ_Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEI/XML"
      ],
      "metadata": {
        "id": "T3L1j4ihvKFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GrobId-TEI-XML-Parser installieren\n",
        "# Siehe: https://pypi.org/project/grobid-tei-xml/\n",
        "!pip install grobid_tei_xml"
      ],
      "metadata": {
        "id": "zjGb5J1uhcgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paper-Daten mit GrobId-TEI-XML-Parser auslesen:\n",
        "# So könnte man auch eine \"Rename\"-Funktion implementieren: Autoren und Titel stehen ja jetzt im XML/TEI.\n",
        "import grobid_tei_xml\n",
        "\n",
        "# Für jede Datei im Output-Folder:\n",
        "for file in os.listdir(\"/content/output\"):\n",
        "    # Nur XML-Dateien\n",
        "    if file.endswith(\".xml\"):\n",
        "        # Parsen und Metadaten auslesen:\n",
        "        with open(f\"/content/output/{file}\", 'r') as xml_file:\n",
        "          doc = grobid_tei_xml.parse_document_xml(xml_file.read())\n",
        "          print(\"file name: \" + file)\n",
        "          # Titel\n",
        "          if doc.header.title is not None:\n",
        "            print(\"title: \" + doc.header.title)\n",
        "          # Autoren\n",
        "          if doc.header.authors is not None:\n",
        "            print(\"authors: \" + \", \".join([a.full_name for a in doc.header.authors]))\n",
        "          # DOI\n",
        "          if doc.header.doi is not None:\n",
        "            print(\"doi: \" + str(doc.header.doi))\n",
        "          # Abstract\n",
        "          if doc.abstract is not None:\n",
        "            print(\"abstract: \" + doc.abstract)\n",
        "          print(\"\")\n",
        "          # Der Text des Papers wäre in doc.body, den würde man in irgendeinem NLP-Tool weiterverarbeiten.\n",
        "          #if doc.body is not None:\n",
        "            #print(doc.body)\n",
        "          # Fußnoten\n",
        "          if doc.citations is not None:\n",
        "            for citation in doc.citations:\n",
        "              print(\"citation authors: \" + \", \".join([author.full_name for author in citation.authors]))"
      ],
      "metadata": {
        "id": "TOc5nMiCixrV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"Rename\"-Script"
      ],
      "metadata": {
        "id": "cLen1LCEvPCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"Rename\"\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "for file in os.listdir(\"/content/output\"):\n",
        "  # Nur XML-Dateien\n",
        "  if file.endswith(\".xml\"):\n",
        "      # Parsen und Titel auslesen:\n",
        "      with open(f\"/content/output/{file}\", 'r') as xml_file:\n",
        "        doc = grobid_tei_xml.parse_document_xml(xml_file.read())\n",
        "\n",
        "        # Titel\n",
        "        if doc.header.title is not None:\n",
        "        # Titel bei 250 Zeichen abschneiden, weil sonst zu lange Dateinamen für das Dateisystem. (Linux MAX_FILEPATH = 255)\n",
        "          title = (doc.header.title[:250]) if len(doc.header.title) > 250 else doc.header.title\n",
        "          # Ungültige Dateinamen-Buchstaben löschen. (Da sind zum Teil komische Sonderzeichen in den Titeln, die Linux nicht will.)\n",
        "          title = \"\".join(x for x in title if x.isprintable())\n",
        "        # Wie ist der Pfad zum originalen PDF?\n",
        "        pdf_path = \"/content/input/\" + file.replace(\".tei.xml\", \".pdf\")\n",
        "        \n",
        "        # Umbenennen\n",
        "        shutil.move(pdf_path, f\"/content/input/{title}.pdf\")"
      ],
      "metadata": {
        "id": "tS5FfJPJcXqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy-NLP"
      ],
      "metadata": {
        "id": "sYest1T2vUD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP-Library und englisches Language Model installieren.\n",
        "# spaCy bietet übrigens GPU-Unterstützung, nutzen wir jetzt aber nicht.\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "Ip6TMMRR_Nn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispiel für die Weiterverarbeitung in einer NLP-Library, hier spaCy.\n",
        "# Siehe https://spacy.io/\n",
        "# Wortvektoren, Entity Linking, whatever.\n",
        "# Da kommt noch ziemlich viel Mist raus, aber man würde spacy zum Postprocessing benutzen.\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Kleines englisches Language Model laden\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Für jede Datei im Output-Folder:\n",
        "for file in os.listdir(\"/content/output\"):\n",
        "    # Nur XML-Dateien\n",
        "    if file.endswith(\".xml\"):\n",
        "        with open(f\"/content/output/{file}\", 'r') as xml_file:\n",
        "          doc = grobid_tei_xml.parse_document_xml(xml_file.read())\n",
        "          print(\"file name: \" + file)\n",
        "          if doc.body is not None:\n",
        "            doc = nlp(doc.body)\n",
        "\n",
        "            # Alle Sätze einzeln ausgeben:\n",
        "            for sent in doc.sents:\n",
        "              print(sent)\n",
        "            # Entity Recognition\n",
        "            for entity in doc.ents:\n",
        "              print(entity.text, entity.label_)"
      ],
      "metadata": {
        "id": "C9BLj3kT83J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Volltextsuche: ElasticSearch siehe https://www.elastic.co/elasticsearch/"
      ],
      "metadata": {
        "id": "jV_NOGDcuslN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ElasticSearch Client-Library installieren\n",
        "%%capture\n",
        "\n",
        "!pip install elasticsearch==7.14.0"
      ],
      "metadata": {
        "id": "Gl0JHJZYvY-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Library-Importe\n",
        "try:\n",
        "  import os\n",
        "  import elasticsearch\n",
        "  from elasticsearch import Elasticsearch\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import sys\n",
        "  import json\n",
        "  from ast import literal_eval\n",
        "  from tqdm import tqdm \n",
        "  import datetime\n",
        "  from elasticsearch import helpers\n",
        "  \n",
        "except Exception as e:\n",
        "  print(f\"error: {e}\")"
      ],
      "metadata": {
        "id": "g5YP_GhFvjiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elasticsearch 7.0.0 downloaden und entpacken\n",
        "!wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.0.0-linux-x86_64.tar.gz -q\n",
        "!tar -xzf elasticsearch-7.0.0-linux-x86_64.tar.gz\n",
        "!chown -R daemon:daemon elasticsearch-7.0.0"
      ],
      "metadata": {
        "id": "PWfXdSuBvwy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Daemon-Instanz von Elasticsearch erstellen.\n",
        "import os\n",
        "from subprocess import Popen, PIPE, STDOUT\n",
        "es_server = Popen(['elasticsearch-7.0.0/bin/elasticsearch'], \n",
        "                  stdout=PIPE, stderr=STDOUT,\n",
        "                  preexec_fn=lambda: os.setuid(1)  # als Daemon\n",
        "                 )"
      ],
      "metadata": {
        "id": "KcRWwSq3v_RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dieser Teil ist wichtig: warten. ElasticSearch braucht eine Weile, bis es hochgefahren ist.\n",
        "import time\n",
        "time.sleep(20)"
      ],
      "metadata": {
        "id": "cqjkzw4qwI3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "# Wenn Du hier 1 root und 2 daemon Prozesse siehst, ist Elasticsearch erfolgreich gestartet.\n",
        "ps -ef | grep elasticsearch"
      ],
      "metadata": {
        "id": "7fGADgVRwXUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Antwortet ElasticSearch?\n",
        "!curl -sX GET \"localhost:9200/\""
      ],
      "metadata": {
        "id": "p_0Bb4CSwoZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = Elasticsearch(hosts = [{\"host\":\"localhost\", \"port\":9200}])\n",
        "# Gucken, ob Python den ElasticSearch-Server erreichen kann.\n",
        "es.ping()"
      ],
      "metadata": {
        "id": "GKKzSWilwvhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset erstellen, rudimentär, ist nur ein Showcase.\n",
        "dataset = pd.DataFrame({\"name\": [], \"body\": []})\n",
        "\n",
        "for file in os.listdir(\"/content/output\"):\n",
        "  # Nur XML-Dateien\n",
        "  if file.endswith(\".xml\"):\n",
        "      # Parsen und Body auslesen\n",
        "      with open(f\"/content/output/{file}\", 'r') as xml_file:\n",
        "        doc = grobid_tei_xml.parse_document_xml(xml_file.read())\n",
        "\n",
        "        # Name der Datei (\"name\") und Text des Papers (\"body\") in eine Tabellenform bringen\n",
        "        if doc.body is not None:\n",
        "          row = {\"name\": file, \"body\": doc.body}\n",
        "          dataset = dataset.append(row, ignore_index=True)\n",
        "\n",
        "print(f\"shape of dataset: {dataset.shape}\")\n",
        "# Vorschau:\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "-F3EF9hUw4Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Einstellungen (\"settings\") und Schema (\"mappings\") für den Elasticsearch-Index\n",
        "Settings = {\n",
        "    \"settings\":{\n",
        "        \"number_of_shards\":1,\n",
        "        \"number_of_replicas\":0\n",
        "    },\n",
        "    \"mappings\":{\n",
        "        \"properties\":{\n",
        "            \"name\":{\n",
        "                \"type\":\"text\"\n",
        "            },\n",
        "            \"body\":{\n",
        "                \"type\":\"text\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "JiEJpE3PxSYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generische Funktion, erstellt JSON-formatierte Dictionaries für Elasticsearch.\n",
        "def json_formatter(dataset, index_name, index_type='_doc'):\n",
        "    try:\n",
        "        List = []\n",
        "        columns = dataset.columns\n",
        "        for idx, row in dataset.iterrows():\n",
        "            dic = {}\n",
        "            dic['_index'] = index_name\n",
        "            dic['_type'] = index_type\n",
        "            source = {}\n",
        "            for i in dataset.columns:\n",
        "                source[i] = row[i]\n",
        "            dic['_source'] = source\n",
        "            List.append(dic)\n",
        "        return List\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(\"There is a problem: {}\".format(e))"
      ],
      "metadata": {
        "id": "a9IMzm4yxkRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellt den Index \"paper_index\" in ElasticSearch mit den Einstellungen, die wir oben definiert haben.\n",
        "PAPER_INDEX = es.indices.create(index=\"paper_index\", ignore=[400,404], body=Settings)\n",
        "PAPER_INDEX"
      ],
      "metadata": {
        "id": "BQB0jqfbx2JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON-formatiertes Dataset für ElasticSearch\n",
        "json_Formatted_dataset = json_formatter(dataset=dataset, index_name='paper_index', index_type='_doc')\n",
        "# Vorschau:\n",
        "json_Formatted_dataset[0]"
      ],
      "metadata": {
        "id": "B_0-8q6AyJRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Um das Dataset in ElasticSearch zu importieren, nutzen wir das ElasticSearch Bulk API.\n",
        "try:\n",
        "    res = helpers.bulk(es, json_Formatted_dataset)\n",
        "    print(\"Successfully imported to ElasticSearch.\")\n",
        "except Exception as e:\n",
        "    print(f\"error: {e}\")"
      ],
      "metadata": {
        "id": "pc1ZX2OwyQMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Suche nach \"LIDAR\"\n",
        "query = es.search(\n",
        "    index=\"paper_index\",\n",
        "    body={\n",
        "        \"size\":20,\n",
        "        \"query\":{\n",
        "            \"bool\":{\n",
        "                \"must\":[\n",
        "                        {\"match\":{\"body\":\"LIDAR\"}}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "output = pd.json_normalize((query['hits']['hits']))\n",
        "output"
      ],
      "metadata": {
        "id": "nGzL5YNFyl7r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}